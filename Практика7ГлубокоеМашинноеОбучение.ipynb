{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M3kb1sNeFurS",
        "outputId": "dcd9eae1-d940-43f5-f6a6-8759e60bac3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Переоценка и недооценка**\n",
        "В этом блокноте собраны примеры кода из главы 3, раздела 6 книги «Глубокое обучение с помощью Python». Обратите внимание, что в оригинальном тексте содержится гораздо больше информации, в частности, дополнительные пояснения и рисунки: в этом блокноте вы найдете только исходный код и соответствующие комментарии.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dNrbkPy1GHeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Во всех примерах, которые мы рассматривали в предыдущей главе - предсказание настроения в рецензиях на фильмы, классификация тем и регрессия цен на жилье - мы могли заметить, что производительность нашей модели на удержанных проверочных данных всегда достигала пика через несколько эпох, а затем начинала ухудшаться, то есть наша модель быстро начинала перестраиваться под обучающие данные. Переоптимизация происходит в каждой задаче машинного обучения. Научиться справляться с избыточной подгонкой очень важно для освоения машинного обучения.\n",
        "\n",
        "Фундаментальная проблема машинного обучения - это противоречие между оптимизацией и обобщением. Под «оптимизацией» понимается процесс настройки модели для достижения наилучшей производительности на обучающих данных («обучение» в слове «машинное обучение»), а под «обобщением» - то, насколько хорошо обученная модель будет работать на данных, которые она никогда раньше не видела. Конечно, цель игры - добиться хорошего обобщения, но вы не контролируете обобщение; вы можете только корректировать модель на основе ее обучающих данных.\n",
        "\n",
        "В начале обучения оптимизация и обобщение коррелируют: чем меньше ваши потери на тренировочных данных, тем меньше ваши потери на тестовых данных. Пока это происходит, ваша модель считается недоукомплектованной: еще есть прогресс, сеть еще не смоделировала все релевантные паттерны в обучающих данных. Но после определенного количества итераций на обучающих данных обобщение перестает улучшаться, показатели валидации останавливаются, а затем начинают ухудшаться: модель начинает чрезмерно подходить, то есть она начинает изучать паттерны, характерные для обучающих данных, но вводящие в заблуждение или не имеющие отношения к новым данным.\n",
        "\n",
        "Чтобы предотвратить обучение модели на ошибках или нерелевантных паттернах, найденных в обучающих данных, лучшим решением, конечно же, является получение большего количества обучающих данных. Модель, обученная на большем количестве данных, естественно, будет лучше обобщать. Когда это уже невозможно, следующим лучшим решением будет изменение количества информации, которую модели разрешено хранить, или добавление ограничений на то, какую информацию ей разрешено хранить. Если сеть может позволить себе запомнить только небольшое количество паттернов, процесс оптимизации заставит ее сосредоточиться на наиболее заметных паттернах, которые имеют больше шансов на хорошее обобщение.\n",
        "\n",
        "Такая борьба с избыточным соответствием называется регуляризацией. Давайте рассмотрим некоторые из наиболее распространенных методов регуляризации и применим их на практике, чтобы улучшить нашу модель классификации фильмов из предыдущей главы.\n",
        "\n",
        "Примечание: в этом блокноте мы будем использовать тестовый набор IMDB в качестве валидационного набора. В данном контексте это не имеет значения.\n",
        "\n",
        "Давайте подготовим данные, используя код из главы 3, раздел 5:\n"
      ],
      "metadata": {
        "id": "WYcf8L-LGr9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Создаем полностью нулевую матрицу формы (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # устанавливаем определенные индексы результатов[i] в 1s\n",
        "    return results\n",
        "\n",
        "# Наши векторизованные обучающие данные\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Наши векторизованные тестовые данные\n",
        "x_test = vectorize_sequences(test_data)\n",
        "# Наши векторизованные метки\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "vn557XA3F9-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Борьба с чрезмерной подгонкой**\n"
      ],
      "metadata": {
        "id": "tIh5lMmqHDTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Уменьшение размера сети**\n",
        "\n",
        "Самый простой способ предотвратить перебор - уменьшить размер модели, то есть количество обучаемых параметров в модели (которое определяется количеством слоев и количеством блоков в слое). В глубоком обучении количество обучаемых параметров в модели часто называют «мощностью» модели. Интуитивно понятно, что модель с большим количеством параметров будет обладать большей «способностью к запоминанию» и, следовательно, сможет легко выучить идеальное словарное отображение между обучающими образцами и их целями - отображение без какой-либо обобщающей способности. Например, модель с 500 000 двоичных параметров можно легко заставить выучить класс каждой цифры в обучающем наборе MNIST: нам понадобится всего 10 двоичных параметров для каждой из 50 000 цифр. Такая модель будет бесполезна для классификации новых образцов цифр. Всегда помните об этом: модели глубокого обучения, как правило, хорошо подстраиваются под обучающие данные, но реальная задача заключается в обобщении, а не в подгонке.\n",
        "\n",
        "С другой стороны, если сеть имеет ограниченные ресурсы запоминания, она не сможет выучить это отображение так же легко, и поэтому, чтобы минимизировать потери, ей придется прибегнуть к обучению сжатых представлений, которые обладают предсказательной силой в отношении целей - именно тот тип представлений, который нас интересует. В то же время не забывайте, что вы должны использовать модели с достаточным количеством параметров, чтобы они не были недоукомплектованы: ваша модель не должна испытывать недостатка в ресурсах для запоминания. Необходимо найти компромисс между «слишком большой емкостью» и «недостаточной емкостью».\n",
        "\n",
        "К сожалению, не существует волшебной формулы, позволяющей определить нужное количество слоев или размер каждого из них. Вам придется оценить множество различных архитектур (разумеется, на валидационном, а не на тестовом множестве), чтобы найти правильный размер модели для ваших данных. Общий порядок действий для поиска подходящего размера модели заключается в том, чтобы начать с относительно небольшого количества слоев и параметров и увеличивать размер слоев или добавлять новые слои до тех пор, пока вы не увидите уменьшающуюся отдачу в отношении потерь при валидации.\n",
        "\n",
        "Давайте попробуем это сделать на нашей сети классификации кинорецензий. Наша первоначальная сеть выглядела следующим образом:"
      ],
      "metadata": {
        "id": "zDMq5h1lHKqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "original_model = models.Sequential()\n",
        "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "original_model.add(layers.Dense(16, activation='relu'))\n",
        "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "original_model.compile(optimizer='rmsprop',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['acc'])"
      ],
      "metadata": {
        "id": "JqnqPzWmG_lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь попробуем заменить ее на эту меньшую сеть:"
      ],
      "metadata": {
        "id": "BrNF2xCOH2B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smaller_model = models.Sequential()\n",
        "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
        "smaller_model.add(layers.Dense(4, activation='relu'))\n",
        "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "smaller_model.compile(optimizer='rmsprop',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])"
      ],
      "metadata": {
        "id": "fm_AyL1KH0Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот сравнение потерь при валидации исходной сети и сети меньшего размера. Точки - это значения потерь при валидации меньшей сети, а крестики - исходной сети (помните: меньшие потери при валидации свидетельствуют о лучшей модели)."
      ],
      "metadata": {
        "id": "N_AYOhOVH9JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_hist = original_model.fit(x_train, y_train,\n",
        "                                   epochs=20,\n",
        "                                   batch_size=512,\n",
        "                                   validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6k1Zjs8H4cU",
        "outputId": "276e43da-50f4-4844-d5d6-a9bb289d95c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 6s 109ms/step - loss: 0.4884 - acc: 0.8042 - val_loss: 0.3567 - val_acc: 0.8768\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.2859 - acc: 0.9003 - val_loss: 0.2934 - val_acc: 0.8853\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 3s 52ms/step - loss: 0.2202 - acc: 0.9214 - val_loss: 0.2783 - val_acc: 0.8894\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 3s 52ms/step - loss: 0.1851 - acc: 0.9337 - val_loss: 0.2855 - val_acc: 0.8861\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.1620 - acc: 0.9418 - val_loss: 0.3008 - val_acc: 0.8814\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 0.1433 - acc: 0.9500 - val_loss: 0.3351 - val_acc: 0.8719\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1292 - acc: 0.9541 - val_loss: 0.3270 - val_acc: 0.8778\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 3s 52ms/step - loss: 0.1167 - acc: 0.9606 - val_loss: 0.3431 - val_acc: 0.8750\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1034 - acc: 0.9653 - val_loss: 0.3655 - val_acc: 0.8720\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.0952 - acc: 0.9679 - val_loss: 0.3817 - val_acc: 0.8710\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.0843 - acc: 0.9716 - val_loss: 0.4116 - val_acc: 0.8658\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 3s 53ms/step - loss: 0.0747 - acc: 0.9758 - val_loss: 0.4342 - val_acc: 0.8640\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 0.0683 - acc: 0.9784 - val_loss: 0.4737 - val_acc: 0.8618\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 3s 52ms/step - loss: 0.0608 - acc: 0.9810 - val_loss: 0.4775 - val_acc: 0.8623\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.0532 - acc: 0.9844 - val_loss: 0.4960 - val_acc: 0.8628\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 3s 52ms/step - loss: 0.0479 - acc: 0.9863 - val_loss: 0.5303 - val_acc: 0.8580\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 3s 53ms/step - loss: 0.0399 - acc: 0.9900 - val_loss: 0.5677 - val_acc: 0.8584\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 48ms/step - loss: 0.0369 - acc: 0.9907 - val_loss: 0.6212 - val_acc: 0.8515\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.0324 - acc: 0.9916 - val_loss: 0.6103 - val_acc: 0.8585\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.0264 - acc: 0.9938 - val_loss: 0.6268 - val_acc: 0.8585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
        "                                       epochs=20,\n",
        "                                       batch_size=512,\n",
        "                                       validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2myu2IODH8dj",
        "outputId": "bd5a3a9a-6c19-4b08-ce93-e4c6231eae05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 6s 114ms/step - loss: 0.6265 - acc: 0.6754 - val_loss: 0.5748 - val_acc: 0.7465\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 3s 55ms/step - loss: 0.5072 - acc: 0.8408 - val_loss: 0.4660 - val_acc: 0.8526\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.3821 - acc: 0.8936 - val_loss: 0.3602 - val_acc: 0.8823\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.2880 - acc: 0.9119 - val_loss: 0.3081 - val_acc: 0.8870\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.2370 - acc: 0.9217 - val_loss: 0.2856 - val_acc: 0.8885\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.2069 - acc: 0.9281 - val_loss: 0.2782 - val_acc: 0.8902\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.1864 - acc: 0.9362 - val_loss: 0.2826 - val_acc: 0.8845\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 3s 61ms/step - loss: 0.1705 - acc: 0.9432 - val_loss: 0.2808 - val_acc: 0.8876\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.1574 - acc: 0.9474 - val_loss: 0.2885 - val_acc: 0.8849\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1459 - acc: 0.9516 - val_loss: 0.2987 - val_acc: 0.8832\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.1358 - acc: 0.9552 - val_loss: 0.3019 - val_acc: 0.8829\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1267 - acc: 0.9586 - val_loss: 0.3112 - val_acc: 0.8808\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1187 - acc: 0.9618 - val_loss: 0.3229 - val_acc: 0.8797\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.1117 - acc: 0.9648 - val_loss: 0.3324 - val_acc: 0.8778\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1042 - acc: 0.9678 - val_loss: 0.3434 - val_acc: 0.8757\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0975 - acc: 0.9696 - val_loss: 0.3596 - val_acc: 0.8742\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.0913 - acc: 0.9716 - val_loss: 0.3700 - val_acc: 0.8740\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.0855 - acc: 0.9741 - val_loss: 0.3830 - val_acc: 0.8714\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.0802 - acc: 0.9759 - val_loss: 0.3966 - val_acc: 0.8698\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.0749 - acc: 0.9782 - val_loss: 0.4114 - val_acc: 0.8678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, 21)\n",
        "original_val_loss = original_hist.history['val_loss']\n",
        "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
      ],
      "metadata": {
        "id": "r8j_4TAlICaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# b+ означает «синий крест»\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "# «bo» означает «синяя точка»\n",
        "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "0Ak3SQNJIKHL",
        "outputId": "6082a8ce-ba21-47b1-96fe-3e5851abd19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видим, меньшая сеть начинает перестраиваться позже, чем эталонная (после 6 эпох, а не 4), и ее производительность снижается гораздо медленнее, как только она начинает перестраиваться.\n",
        "\n",
        "А теперь, ради интереса, добавим к этому эталону сеть, которая имеет гораздо большую емкость, гораздо большую, чем того требует задача:"
      ],
      "metadata": {
        "id": "RD5dLEqaISbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_model = models.Sequential()\n",
        "bigger_model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
        "bigger_model.add(layers.Dense(512, activation='relu'))\n",
        "bigger_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "bigger_model.compile(optimizer='rmsprop',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['acc'])"
      ],
      "metadata": {
        "id": "aGRpoJxEIY83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_model_hist = bigger_model.fit(x_train, y_train,\n",
        "                                     epochs=20,\n",
        "                                     batch_size=512,\n",
        "                                     validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dxxO7OGIdLK",
        "outputId": "d3772ab0-e3aa-44f2-fc11-ea8579aa8708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.4929 - acc: 0.7634"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот как выглядит большая сеть по сравнению с эталонной. Точки - это значения потерь при проверке большей сети, а крестики - исходной сети."
      ],
      "metadata": {
        "id": "glL-RGU4ImJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_model_val_loss = bigger_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, bigger_model_val_loss, 'bo', label='Bigger model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PaKSES0EImah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Более крупная сеть начинает перестраиваться почти сразу же, после одной эпохи, и перестраивается гораздо сильнее. Ее потери при проверке также более шумные.\n",
        "\n",
        "А вот потери при обучении для двух наших сетей:"
      ],
      "metadata": {
        "id": "QCdQJ6oJJAdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_train_loss = original_hist.history['loss']\n",
        "bigger_model_train_loss = bigger_model_hist.history['loss']\n",
        "\n",
        "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, bigger_model_train_loss, 'bo', label='Bigger model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B8j_7WRQJCrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видите, более крупная сеть очень быстро приближает потери при обучении к нулю. Чем больше мощность сети, тем быстрее она сможет смоделировать обучающие данные (что приводит к низким потерям при обучении), но тем больше она подвержена перестройке (что приводит к большой разнице между потерями при обучении и проверке)."
      ],
      "metadata": {
        "id": "U-7JI3eUJVbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Добавление весовой регуляризации**"
      ],
      "metadata": {
        "id": "HKzmWvDwJZyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возможно, вы знакомы с принципом «бритвы Оккама»: при наличии двух объяснений чего-либо наиболее вероятным правильным будет «простейшее» объяснение, то есть то, которое делает наименьшее количество предположений. Это относится и к моделям, обучаемым нейронными сетями: при наличии обучающих данных и архитектуры сети существует несколько наборов значений весов (несколько моделей), которые могут объяснить данные, и более простые модели менее склонны к перестройке, чем сложные.\n",
        "\n",
        "Простая модель» в данном контексте - это модель, в которой распределение значений параметров имеет меньшую энтропию (или модель с меньшим количеством параметров, как мы видели в разделе выше). Таким образом, распространенным способом борьбы с избыточной подгонкой является наложение ограничений на сложность сети путем принуждения ее весов принимать только малые значения, что делает распределение значений весов более «регулярным». Это называется «регуляризацией весов», и делается она путем добавления к функции потерь сети стоимости, связанной с наличием больших весов. Эта стоимость бывает двух видов:\n",
        "\n",
        "- Регуляризация L1, при которой добавленная стоимость пропорциональна абсолютному значению весовых коэффициентов (т.е. тому, что называется «нормой L1» весов).\n",
        "- Регуляризация L2, при которой добавленная стоимость пропорциональна квадрату значения весовых коэффициентов (т.е. тому, что называется «нормой L2» весов). В контексте нейронных сетей L2-регуляризация также называется затуханием веса. Пусть вас не смущает другое название: с математической точки зрения затухание веса - это то же самое, что и L2-регуляризация.\n",
        "\n",
        "В Keras весовая регуляризация добавляется путем передачи экземпляров весового регуляризатора слоям в качестве аргументов ключевых слов. Давайте добавим весовую регуляризацию L2 в нашу сеть классификации рецензий на фильмы:"
      ],
      "metadata": {
        "id": "1B5CVByFJbOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "\n",
        "l2_model = models.Sequential()\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu', input_shape=(10000,)))\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu'))\n",
        "l2_model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "qQQsdCPLJPlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2_model.compile(optimizer='rmsprop',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['acc'])"
      ],
      "metadata": {
        "id": "DFtC0BFcJ0IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "l2(0.001) означает, что каждый коэффициент в матрице весов слоя будет добавлять 0.001 * вес_коэффициента_значения к общему убытку сети. Обратите внимание, что поскольку этот штраф добавляется только во время обучения, потери для этой сети будут гораздо выше во время обучения, чем во время тестирования.\n",
        "Вот влияние нашего штрафа регуляризации L2"
      ],
      "metadata": {
        "id": "lDubbOGOJ6BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l2_model_hist = l2_model.fit(x_train, y_train,\n",
        "                             epochs=20,\n",
        "                             batch_size=512,\n",
        "                             validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYksH49LJ1jl",
        "outputId": "f5694601-45a5-4963-8ef6-45280cf35be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "48/49 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7908"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pe3Yv8J5KHHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видите, модель с регуляризацией L2 (точки) стала гораздо более устойчивой к переборке, чем эталонная модель (крестики), хотя обе модели имеют одинаковое количество параметров.\n",
        "\n",
        "В качестве альтернативы регуляризации L2 можно использовать один из следующих весовых регуляризаторов Keras:"
      ],
      "metadata": {
        "id": "1PtNz_SSKL0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "\n",
        "# L1 regularization\n",
        "regularizers.l1(0.001)\n",
        "\n",
        "# L1 and L2 regularization at the same time\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)"
      ],
      "metadata": {
        "id": "Hcg8m3AhKRdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Добавление dropout**"
      ],
      "metadata": {
        "id": "139XCRbcKR-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout - один из самых эффективных и наиболее часто используемых методов регуляризации для нейронных сетей, разработанный Хинтоном и его студентами в Университете Торонто. Dropout, применяемый к слою, заключается в случайном «выпадении» (т. е. установке нуля) ряда выходных признаков слоя в процессе обучения. Допустим, данный слой в процессе обучения обычно возвращал вектор [0.2, 0.5, 1.3, 0.8, 1.1] для заданного входного образца; после применения отсева этот вектор будет содержать несколько нулевых записей, распределенных случайным образом, например [0, 0.5, 1.3, 0, 1.1]. Коэффициент отсева» - это доля признаков, которые обнуляются; обычно он устанавливается в диапазоне от 0,2 до 0,5. В момент тестирования никакие единицы не отбрасываются, а вместо этого выходные значения слоя уменьшаются на коэффициент, равный коэффициенту отсева, чтобы сбалансировать тот факт, что активными являются больше единиц, чем в момент обучения.\n",
        "\n",
        "Рассмотрим матрицу Numpy, содержащую выход слоя, layer_output , формы (batch_size, features). Во время обучения мы будем случайным образом обнулять часть значений в матрице:"
      ],
      "metadata": {
        "id": "G1dwEJoRKVUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Во время обучения: мы отбрасываем 50% единиц на выходе\n",
        "layer_output *= np.randint(0, high=2, size=layer_output.shape)"
      ],
      "metadata": {
        "id": "slSeYHMjKU_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Во время тестирования мы уменьшим результат на коэффициент dropout. Здесь мы масштабируем на 0,5 (потому что ранее мы отсеяли половину единиц):"
      ],
      "metadata": {
        "id": "SY4ivjLCKqjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Во время тестирования:\n",
        "layer_output *= 0.5"
      ],
      "metadata": {
        "id": "NPytFE2rKt_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, что этот процесс можно реализовать, выполнив обе операции во время обучения и оставив выход неизменным во время тестирования, что часто и происходит на практике:"
      ],
      "metadata": {
        "id": "WOWcxBc3K3dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Во время обучения:\n",
        "layer_output *= np.randint(0, high=2, size=layer_output.shape)\n",
        "# Обратите внимание, что в данном случае мы масштабируем *вверх*, а не *вниз*.\n",
        "layer_output /= 0.5"
      ],
      "metadata": {
        "id": "rMM_i4DVK7HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эта техника может показаться странной и произвольной. Почему это должно помочь уменьшить избыточную подгонку? Джефф Хинтон сказал, что его вдохновил, среди прочего, механизм предотвращения мошенничества, используемый банками - по его собственным словам: «Я пошел в свой банк. Кассиры постоянно менялись, и я спросил одного из них, почему. Он ответил, что не знает, но их часто переставляют с места на место. Я решил, что это, должно быть, потому, что для успешного обмана банка требуется сотрудничество между сотрудниками. Это заставило меня понять, что случайное удаление различных подмножеств нейронов в каждом примере позволит избежать сговора и, таким образом, уменьшить перебор».\n",
        "\n",
        "Основная идея заключается в том, что введение шума в выходные значения слоя может разрушить случайные паттерны, которые не являются значимыми (то, что Хинтон называет «заговорами»), и которые сеть начала бы запоминать, если бы шум не присутствовал.\n",
        "\n",
        "В Keras вы можете ввести отсев в сеть с помощью слоя Dropout, который применяется к выходу слоя, расположенного непосредственно перед ним, например:"
      ],
      "metadata": {
        "id": "gbQizZy9K-jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Dropout(0.5))"
      ],
      "metadata": {
        "id": "PxjihqPxK-WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте добавим два слоя Dropout в нашу сеть IMDB, чтобы посмотреть, насколько хорошо они справляются с задачей уменьшения избыточной подгонки:"
      ],
      "metadata": {
        "id": "t9ysOAD4LPt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpt_model = models.Sequential()\n",
        "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(16, activation='relu'))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "dpt_model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])"
      ],
      "metadata": {
        "id": "gQlK3Xp1LSr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
        "                               epochs=20,\n",
        "                               batch_size=512,\n",
        "                               validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "eERfb0CdLUxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим график результатов:"
      ],
      "metadata": {
        "id": "DZbMWJEaLd1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zCjswm0NLWk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "И снова явное улучшение по сравнению с эталонной сетью.\n",
        "\n",
        "Подведем итог: вот наиболее распространенные способы предотвращения оверфиттинга в нейронных сетях:\n",
        "- Получение большего количества обучающих данных.\n",
        "- Уменьшение пропускной способности сети.\n",
        "- Добавление регуляризации веса.\n",
        "- Добавление dropout."
      ],
      "metadata": {
        "id": "r6MUhjM4LiUn"
      }
    }
  ]
}